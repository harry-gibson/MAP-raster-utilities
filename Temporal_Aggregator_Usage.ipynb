{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal aggregation\n",
    "\n",
    "This is a demonstration of how to run temporal aggregations using the cython library code. (Or other reduction-type aggregations of multiple files into one - not specifically temporal).\n",
    "\n",
    "The code is written in cython (`raster_utilities/aggregation/temporal/core/temporal.pyx`) and a helper class `raster_utilities/aggregation/temporal/temporal_aggregation_runner.py` is provided to assist with loading the data and passing it to the core function.\n",
    "\n",
    "This notebook demonstrates how to use the helper class, by building the input arguments that it needs and then calling its main run method.\n",
    "\n",
    "Import the aggregation helper class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from raster_utilities.aggregation.temporal.TemporalAggregator import TemporalAggregator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define the aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"temporal\" aggregation is controlled by a dictionary in which the keys represent the required output aggregation points (years, calendar months, etc, or just a single key for a quick summary of \"everything\") and the values are a list of files corresponding to that period. For each dictionary item, the files in the list will be summarised / flattened into a single output file, for each requested statistic type. \n",
    "\n",
    "A given input file can appear in more than one output aggregation (e.g. you can have keys for real months and for synoptic months) - each key is processed separately, except for if synoptic output is requested in which case all files mentioned in any of the dictionary items will contribute (but only once).\n",
    "\n",
    "Here we show a couple of ways to build that object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1: MODIS 8-daily images to dynamic monthly, dynamic annual, and synoptic monthly outputs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines a function to build the dictionary from a list of MODIS 8-daily filenames, based on extracting the date from the filenames, and will create a dictionary to output up to all of dynamic monthly, dynamic annual, and synoptic monthly, and synoptic overall in one pass. \n",
    "\n",
    "Note: synoptic overall we can alternatively ask the aggregator to do on-the-fly, as opposed to adding a key to the dictionary for it (containing all files: doSynopticOverall=True). No need to do both! The aggregator can only do it on the fly if it \"sees\" all the files; we can't do it that way with parallel processing.\n",
    "\n",
    "This could be changed to suit the filename patterns being used and the type of outputs we want (annual, monthly, synoptic months?) \n",
    "\n",
    "We can use a defaultdict rather than a real dict which simplifies the loop a bit.\n",
    "\n",
    "The string keys of the dictionary will be used to create the output filenames; we pass in a string \"tag\" which will be used in conjunction with the date to generate these keys. You might want to alter the strings slightly to make them more informative. \n",
    "\n",
    "There is scope to be more efficient here by sorting the dynamic monthly requests and then making a new aggregator for each one, using its synoptic-overall functionality to effectively generate the annual ones for free at the same time as the monthly ones. This would need modification to TemporalAggregator or bespoke handling of the filenames here.\n",
    "\n",
    "This version of the function parses filenames that are in the new 6-token filename format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildMODISKeyFromMGDailies(fileList, doMonthly=True, doAnnual=True, doSynopticMonthly=True, doSynopticOverall=False):\n",
    "    # mapping of julian day to number of the calendar month\n",
    "    daymonths = {1:1, 9:1, 17:1, 25:1, 33:2, 41:2, 49:2, 57:2, 65:3, 73:3, 81:3, 89:3, 97:4, \n",
    "             105:4, 113:4, 121:5, 129:5, 137:5, 145:5, 153:6, 161:6, 169:6, 177:6, 185:7, \n",
    "             193:7, 201:7, 209:7, 217:8, 225:8, 233:8, 241:8, 249:9, 257:9, 265:9, 273:9, \n",
    "             281:10, 289:10, 297:10, 305:11, 313:11, 321:11, 329:11, 337:12, 345:12, 353:12, \n",
    "             361:12}\n",
    "             \n",
    "    processingKey = defaultdict(list)\n",
    "    fnTemplate = \"{}.{}.{}.{}.{}.{}\"\n",
    "    statPlaceholder = \"*\"\n",
    "    if tag is not None:\n",
    "        if not tag.endswith(\".\"):\n",
    "            tag = tag + \".\"\n",
    "    for fn in fileList:\n",
    "        parts = os.path.basename(fn).split('.')\n",
    "        assert len(parts)==7\n",
    "        var, yr, daynum, _, res, spatialstat, tif = parts\n",
    "        assert len(daynum) == 3\n",
    "        monthStr = str(daymonths[int(daynum)]).zfill(2)\n",
    "        if doMonthly:\n",
    "            outKey = fnTemplate.format(var,yr,monthStr,statPlaceholder,res,spatialstat)\n",
    "            processingKey[outKey].append(fn)\n",
    "        if doAnnual:\n",
    "            outKey = fnTemplate.format(var,yr,\"Annual\",statPlaceholder,res,spatialstat)\n",
    "            processingKey[outKey].append(fn)\n",
    "        if doSynopticMonthly:\n",
    "            outKey = fnTemplate.format(var,\"Synoptic\",monthStr,statPlaceholder,res,spatialstat)\n",
    "            outKeyOverall = fnTemplate.format(var,\"Synoptic\",\"Overall\",statPlaceholder,res,spatialstat)\n",
    "            processingKey[outKey].append(fn)\n",
    "            if doSynopticOverall:\n",
    "                processingKey[outKeyOverall].append(fn)\n",
    "    return processingKey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version does the same but parses filenames that are in the original format for the MODIS 8-daily grids (e.g. A2000049_xxx.tif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildMODISKeyFromDailies(tag, fileList, doMonthly=True, doAnnual=True, doSynopticMonthly=True, doSynopticOverall=False):\n",
    "    #daymonths = {1:1, 9:1, 17:1, 25:1, 33:2, 41:2, 49:2, 57:2, 65:3, 73:3, 81:3, 89:3, 97:4, \n",
    "    #         105:4, 113:4, 121:4, 129:5, 137:5, 145:5, 153:6, 161:6, 169:6, 177:6, 185:7, \n",
    "    #         193:7, 201:7, 209:7, 217:8, 225:8, 233:8, 241:8, 249:9, 257:9, 265:9, 273:9, \n",
    "    #         281:10, 289:10, 297:10, 305:10, 313:11, 321:11, 329:11, 337:12, 345:12, 353:12, \n",
    "    #         361:12}\n",
    "    # mapping of julian day to number of the calendar month\n",
    "    daymonths = {1:1, 9:1, 17:1, 25:1, 33:2, 41:2, 49:2, 57:2, 65:3, 73:3, 81:3, 89:3, 97:4, 105:4, 113:4, 121:5, \n",
    "             129:5, 137:5, 145:5, 153:6, 161:6, 169:6, 177:6, 185:7, 193:7, 201:7, 209:7, 217:8, 225:8, 233:8, \n",
    "             241:8, 249:9, 257:9, 265:9, 273:9, 281:10, 289:10, 297:10, 305:11, 313:11, 321:11, 329:11, 337:12, \n",
    "             345:12, 353:12, 361:12}\n",
    "             \n",
    "    processingKey = defaultdict(list)\n",
    "    fnTemplate = \"{}.{}.{}.{}.{}.{}\"\n",
    "    statPlaceholder = \"*\"\n",
    "    res = '1km'\n",
    "    spatialstat = 'Data'\n",
    "    for fn in fileList:\n",
    "        parts = os.path.basename(fn).split('_')\n",
    "        dateStr = parts[0]\n",
    "        yr = dateStr[1:5]\n",
    "        daynum = int(dateStr[5:8])\n",
    "        monthStr = str(daymonths[daynum]).zfill(2)\n",
    "        if doMonthly:\n",
    "            outKey = fnTemplate.format(tag,yr,monthStr,statPlaceholder,res,spatialstat)\n",
    "            processingKey[outKey].append(fn)\n",
    "        if doAnnual:\n",
    "            outKey = fnTemplate.format(tag,yr,\"Annual\",statPlaceholder,res,spatialstat)\n",
    "            processingKey[outKey].append(fn)\n",
    "        if doSynopticMonthly:\n",
    "            outKey = fnTemplate.format(tag,\"Synoptic\",monthStr,statPlaceholder,res,spatialstat)\n",
    "            outKeyOverall = fnTemplate.format(tag,\"Synoptic\",\"Overall\",statPlaceholder,res,spatialstat)\n",
    "            processingKey[outKey].append(fn)\n",
    "            if doSynopticOverall:\n",
    "                processingKey[outKeyOverall].append(fn)\n",
    "    return processingKey\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then: \n",
    "# build a dictionary keyed by year, to create annual outputs\n",
    "inFilePattern1 = r'C:\\temp\\Gapfilling\\LST_Day_mosaic\\1km\\8-daily\\*Data.tif'\n",
    "inFiles = glob.glob(inFilePattern1)\n",
    "tag = \"LST_Day_v6\"\n",
    "fileKey = buildMODISKeyFromDailies(tag, inFiles, doMonthly=True, doAnnual=True, doSynopticMonthly=False, doSynopticOverall=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or\n",
    "inFilePattern1 = r'C:\\Temp\\dataprep\\Haiti\\TCW_Out\\TCW_v6_HTI.*.Data.tif'\n",
    "inFiles = glob.glob(inFilePattern1)\n",
    "tag = \"TCW_Filled_v6_HTI\"\n",
    "fileKey = buildMODISKeyFromMGDailies(tag, inFiles, doMonthly=True, doAnnual=True, doSynoptic=False, doOverall=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileKey.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2: balanced means\n",
    "This cell would build a dictionary with a single key, to create a \"balanced\" mean from pre-existing synoptic monthly mean files (created using the cell above and subsequently renamed to the 6-token syntax). We don't pass in a tag, we extract the existing one instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildSynopticBalancedMeanKey(fileList):\n",
    "    tag = None\n",
    "    stat = None\n",
    "    files = []\n",
    "    for fn in fileList:\n",
    "        parts = os.path.basename(fn).split(\".\")\n",
    "        thistag = parts[0]\n",
    "        synoptictag = parts[1]\n",
    "        monthtag = parts[2]\n",
    "        stattag = parts[3]\n",
    "        if tag is None:\n",
    "            tag = thistag\n",
    "        if tag != thistag:\n",
    "            assert False\n",
    "        if synoptictag != \"Synoptic\":\n",
    "            assert False\n",
    "        try:\n",
    "            i = int(monthtag)\n",
    "        except:\n",
    "            continue # the \".Overall\" one\n",
    "        if stat is None:\n",
    "            stat = stattag\n",
    "        if stat != stattag:\n",
    "            assert False\n",
    "        files.append(fn)\n",
    "    outname = tag + \".\" + \"Synoptic.Overall.Balanced-\" + stat\n",
    "    return {outname: files}\n",
    "\n",
    "inFilePattern = r'C:\\Temp\\dataprep\\EVI\\EVI_Unfilled_Synoptic\\EVI*.Synoptic.*.mean.*.tif'\n",
    "inFiles = glob.glob(inFilePattern)\n",
    "inFiles = [f for f in inFiles if len(f.split('.')[2])==2]\n",
    "fileKey = buildSynopticBalancedMeanKey(inFiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 3: CHIRPS monthlies to dynamic annual outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildBasicKey(fileList):\n",
    "    processingKey = defaultdict(list)\n",
    "    for fn in fileList:\n",
    "        parts = os.path.basename(fn).split('.')\n",
    "        yr = parts[1]\n",
    "        outkey = \"CHIRPS.\"+yr\n",
    "        processingKey[outkey].append(fn)\n",
    "    return processingKey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 4: \n",
    "just some kind of one-off thing, make a single output by definining the files against a one key dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files = glob.glob(r'J:\\Temp_Suitability\\5k\\Pf\\monthly_pf\\*.2002.*.tif')\n",
    "fileKey = {\"test-2002\": files}\n",
    "fileKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monthlyToSynopticAndAnnual(fileList, doSynopticMonthly=True, doAnnual=True, doSynopticOverall=True):\n",
    "    processingKey = defaultdict(list)\n",
    "    for fn in fileList:\n",
    "        parts = os.path.basename(fn).split('.')\n",
    "        assert len(parts)==7\n",
    "        var,yr,mth,timeAgg,res,resAgg,tif = parts\n",
    "        assert len(mth)==2\n",
    "        assert len(yr)==4\n",
    "        outKey = \".\".join([var, \"Synoptic\", mth])\n",
    "        outKeyOverall = \".\".join([var, \"Synoptic\", \"Overall\"])\n",
    "        outKeyAnnual = \".\".join([var, yr, \"Annual\"])\n",
    "        if doSynopticMonthly:\n",
    "            processingKey[outKey].append(fn)\n",
    "        if doSynopticOverall:\n",
    "            processingKey[outKeyOverall].append(fn)\n",
    "        if doAnnual:\n",
    "            processingKey[outKeyAnnual].append(fn)\n",
    "    return processingKey\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(r'\\\\map-fs1.ndph.ox.ac.uk\\map_data\\temp\\MODIS_Global\\MOD11A2_v6_LST\\Air_temp_min\\1km\\Monthly\\*.tif')\n",
    "fileKey = monthlyToSynopticAndAnnual(files, doSynopticMonthly=True, doAnnual=False, doSynopticOverall=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Other setup\n",
    "\n",
    "We also need to specify the output folder, the output nodata value, and whether we want to create a synoptic (overall) output too (this doubles memory use so don't do unless you need it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outDir = r\"G:\\modis\\mcd43b4_v5\\TCW_Synoptic_From_5KDaily\"\n",
    "outDir = r\"C:\\temp\\Gapfilling\\LST_Day_mosaic\\1km\\Monthly\"\n",
    "outNDV = -9999\n",
    "doSynoptic = False\n",
    "#outDir = r'J:\\Temp_Suitability\\5k'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we need to specify which stats to do, what's appropriate will depend on the data. For rainfall we just want a sum.\n",
    "The values must be specified as a list of values from the TemporalAggregationStats class. You can also use TemporalAggregationStats.ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from raster_utilities.aggregation.aggregation_values import TemporalAggregationStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the string value of the enums e.g.\n",
    "#stats = [ 'mean']#, 'count', 'SD', 'max', 'min']\n",
    "stats = ['mean', 'min', 'max']\n",
    "# or enum objects e.g.\n",
    "# stats = [TemporalAggregationStats.MEAN, TemporalAggregationStats.RANGE]\n",
    "#stats = [TemporalAggregationStats.MEAN, TemporalAggregationStats.MAX, TemporalAggregationStats.MIN, TemporalAggregationStats.SD]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running \n",
    "\n",
    "Now we just need to instantiate the class and run the aggregation. \n",
    "The runner should automatically handle splitting the processing into tiles if the files are too large to fit into memory, although currently it estimates this based on assuming it can use ~40GB RAM so you might need to tweak it directly. Intermediate processing tiles are not automatically deleted at present.\n",
    "\n",
    "doSynoptic here controls whether the aggregator should produce \"grand totals\". If we're aggregating a whole cube, then we can do it like this, or we can simply add an entry to fileKey that contains all the files (using the doSynopticOverall option of the file key helper functions). The aggregator can only do it as grand totals for the files that it sees, so it only works this way if we are not using multiprocessing. (Note - the aggregator tracks filenames so it doesn't matter if the same file is passed to it multiple times e.g. in doing dynamic monthly and dynamic annual outputs.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = TemporalAggregator(fileKey, outDir, outNDV, stats, doSynoptic, bytesLimit=8e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "agg.RunAggregation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stitching the tiles\n",
    "\n",
    "The aggregator should do this automatically but if it fails for some reason you can do something like this to stitch them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess as subp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stat in agg.stats:\n",
    "    for timeKey in agg._timePoints():\n",
    "        wildcard = \"{}.{}.*.tif\".format(timeKey,stat.value)\n",
    "        tiles = glob.glob(os.path.join(agg._tileFolder, wildcard))\n",
    "        tilesStr = \" \".join(tiles)\n",
    "        tifName = wildcard.replace(\"*.\", \"1km.Data.\")\n",
    "        vrtName = tifName.replace(\".tif\",\".vrt\")\n",
    "        vrtFile = os.path.join(outDir,vrtName)\n",
    "        tifFile = os.path.join(outDir,tifName)\n",
    "        #print (vrtName)\n",
    "        vrtCommand = \"gdalbuildvrt {} {}\".format(os.path.join(outDir,vrtName), tilesStr)\n",
    "        print(vrtCommand)\n",
    "        !{vrtCommand}\n",
    "        transCommand = \"gdal_translate -of GTiff -co COMPRESS=LZW \"+\\\n",
    "            \"-co PREDICTOR=2 -co TILED=YES -co BIGTIFF=YES \" +\\\n",
    "            \"-co NUM_THREADS=ALL_CPUS --config GDAL_CACHEMAX 8000 {} {}\".format(\n",
    "            vrtFile, tifFile)\n",
    "        ovCommand = \"gdaladdo -ro --config COMPRESS_OVERVIEW LZW --config USE_RRD NO \"+\\\n",
    "            \"--config TILED YES --config GDAL_CACHEMAX 8000 {} 2 4 8 16 32 64 128 256\".format(\n",
    "            tifFile)\n",
    "        statCommand = \"gdalinfo -stats {}>nul\".format(tifFile)\n",
    "        print(transCommand)\n",
    "        #subp.check_call(str(cline), shell=True)\n",
    "        subp.check_call(transCommand, shell=True)\n",
    "        print(ovCommand)\n",
    "        subp.check_call(ovCommand, shell=True)\n",
    "        print(statCommand)\n",
    "        subp.check_call(statCommand, shell=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Parallel processing\n",
    "We can do multiple items from the fileKey (i.e. multiple output aggregations) in parallel with the following \n",
    "caveats:\n",
    "- each one still takes a lot of memory and so we can probably do fewer processes than we could with the spatial aggregator, and it doesn't make sense on a desktop machine\n",
    "- in fact the code here won't even work on ipython/windows, the caller function would need to be in a separate .py file - see the spatial aggregator notebook for an example\n",
    "- we can't do an on-the-fly overall synoptic, as each aggregator only gets a subset of files, we have to do a single overall one which will probably take 50% of the time of all the others (monthly + annual) put together\n",
    "- the processing is already multithreaded so we're only speeding up the i/o parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from multiprocessing import Pool\n",
    "fileKey.items()\n",
    "def callAgg(kvp):\n",
    "    try:\n",
    "        oneKeyDict = {kvp[0]:kvp[1]}\n",
    "        agg = TemporalAggregator(oneKeyDict, outDir, outNDV, stats, False, bytesLimit=50e9)\n",
    "        agg.RunAggregation()\n",
    "    except KeyboardInterrupt, e:\n",
    "        pass\n",
    "        \n",
    "def runMulti():\n",
    "    pool = Pool(8)\n",
    "    p = pool.map_async(callAgg, fileKey.items())\n",
    "    try:\n",
    "        r = p.get(0xFFFF)\n",
    "    except KeyboardInterrupt:\n",
    "        print (\"oops\")\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runMulti()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
