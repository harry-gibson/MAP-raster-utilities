{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation (spatial downsampling)\n",
    "\n",
    "### Continuous aggregation\n",
    "\n",
    "The core aggregation code is written in Cython, in `raster_utilities.aggregation.spatial.core.continuous.pyx`. \n",
    "\n",
    "A helper class `raster_utilities.aggregation.spatial.SpatialAggregator` is provided to manage calling the Cython code.\n",
    "\n",
    "This notebook demonstrates using the helper class to aggregate a series of continuous-type raster files (i.e. Float32 values where we want to summarise mean, max, min etc rather than mode, percentage etc in the case of categorical data).\n",
    "\n",
    "The code has been written to read input rasters of theoretically unlimited size, which are read in tiles to build up the output coarser / smaller grids; memory use is determined by the size of the output files (and the number of statistics requested, i.e. number of output files that are created). \n",
    "\n",
    "Note that there is no specific need for the input data to be a .tif. It can be any GDAL-compatible format so long as the data are single-band and 32-bit float type. It has been used to aggregate a global 7-metre resolution grid to mastergrids 1k, reading from a .VRT file to avoid the need to ever generate the mosaiced high-resolution grid as a (huge) tiff file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The helper class\n",
    "from  raster_utilities.aggregation.spatial.SpatialAggregator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Enumerations to provide acceptable values for the aggregation parameters,\n",
    "# avoid having to remember strings\n",
    "from raster_utilities.aggregation.aggregation_values import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a continuous aggregation across a series of files in a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The files to be aggregated should be provided as a list of filepaths. \n",
    "# (Just make a single-item list for one file)\n",
    "#inContFiles = glob.glob(r'H:\\Night\\1km\\Monthly\\LST_Night_v6.*.max.*tif')\n",
    "inContFiles = glob.glob(r'\\\\map-fs1.ndph.ox.ac.uk\\map_data\\mastergrids\\MODIS_Global\\MOD11A2_v6_LST\\Modelled_Air_Temp_Min\\1km\\Synoptic\\*.tif')\n",
    "#inContFiles = glob.glob(r'\\\\map-fs1.ndph.ox.ac.uk\\map_data\\mastergrids\\Other_Global_Covariates\\NightTimeLights\\DMSP_Intercalibrated_Series\\1km\\Annual\\*.tif')\n",
    "\n",
    "# Also provide the output folder\n",
    "#outDir = r'Z:\\mastergrids\\MODIS_Global\\MOD11A2_v6_LST\\LST_Night\\5km\\Monthly'\n",
    "#outDir = r'E:\\Data\\Harry\\Documents\\dial-a-map\\andre_pop'\n",
    "outDir = r'C:\\Temp\\modis_air_min'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the output nodata value (it doesn't have to be the same as the input, incoming NDV will be read from the files (better be set properly!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ndvOut = -9999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the aggregation statistics to create. This must be a list of items from the ContinuousAggregationStats enumeration, or their string representations. \n",
    "\n",
    "The SpatialAggregator determines whether it is supposed to be doing continuous or categorical aggregation based on which stats are requested, and the data type of the files provided must match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# e.g.\n",
    "stats = [ContinuousAggregationStats.MEAN, ContinuousAggregationStats.MAX,\n",
    "         ContinuousAggregationStats.MIN, ContinuousAggregationStats.SD]\n",
    "# stats = [ContinuousAggregationStats.SUM]\n",
    "#stats = [ContinuousAggregationStats.MEAN]\n",
    "# or do do all of them use this convenience: \n",
    "#stats = ContinuousAggregationStats.ALL.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally configure the aggregation. The final parameter for the SpatialAggregator constructor should be a dictionary that configures how the aggregation will run. \n",
    "\n",
    "* There should be a key '`aggregation_type`' that is a member of the AggregationTypes enumeration, i.e. `AggregationTypes.RESOLUTION`, `AggregationTypes.FACTOR`, or `AggregationTypes.SIZE`. \n",
    "* There should be a key 'aggregation_specifier' that determines the output cell size in a manner dependent on the \n",
    "value of aggregation_type as follows:\n",
    "    * `aggregation_type==AggregationTypes.RESOLUTION`: (Float value, or string \"1km\", \"5km\" or \"10km\")\n",
    "    * `aggregation_type==AggregationTypes.FACTOR`: Int value (e.g. 5 to go from 1k rasters to 5k rasters\n",
    "    * `aggregation_type==AggregationTypes.SIZE`: 2-tuple of positive ints specifying the (height,width) of the output rasters\n",
    "* A key \"`resolution_name`\" may be provided, which provides the \"friendly name\" for the output resolution to be used as the fifth token of the 6-token output filenames (e.g. \"5km\")\n",
    "* A key \"`mem_limit_gb`\" may be provided, to limit the memory use (if not provided, 30GB will be the default). Note that it's not very accurate so be conservative!\n",
    "* A key \"`assume_correct_input`\" may be provided; if this is \"`False`\" (by default if not provided) then the input data will be snapped and aligned to a mastergrid template first, before calculating the properties of the output raster\n",
    "* A key \"`sanitise_resolution`\" may be provided; if this is \"`True`\" (by default if not provided) then the output resolution (whether provided numerically or calculated) will be \"sanitised\" to a mastergrid resolution i.e. a value that divides cleanly into 1.0. For example 0.0083334 would become 0.008333333333333 (1/120)).\n",
    "* A key \"`snap_alignment`\" may be provided; if this is `SnapTypes.NEAREST` (the default if not provided) or `SnapTypes.TOWARDS_ORIGIN` then the origin point of the output will be positioned precisely at the top left corner of a cell in a global grid of the requested resolution. Because this potentially moves the extreme (bottom right) point of the output towards the origin such that it is inside the bottom right of the input data, an extra cell will be added if necessary to the output extent to accommodate the full input extent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# e.g.\n",
    "# Resolution can be a floating point number, or a string representing \n",
    "# one of the core mastergrid resolutions \"1km\", \"5km\", or \"10km\".\n",
    "aggArgs = {'aggregation_type': AggregationTypes.RESOLUTION\n",
    "           , 'aggregation_specifier': \"5km\"\n",
    "           , 'resolution_name':'5km'\n",
    "           , 'sanitise_resolution':True\n",
    "           , 'snap_alignment': SnapTypes.NEAREST\n",
    "           , 'assume_correct_input': False\n",
    "           , 'mem_limit_gb': 10\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running - Single-process\n",
    "\n",
    "Now just instantiate and run the aggregation, processing one input file at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "agg = SpatialAggregator(inContFiles, outDir, ndvOut, stats, aggArgs, LogLevels.DEBUG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "agg.RunAggregation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running - multiprocessing\n",
    "\n",
    "Or use multiprocessing to do several files at once - the continuous aggregation algorithm is single-threaded so use multiprocessing instead to make gains. Pick a pool size that corresponds to the number of cores to run at once; keep an eye on disk utilisation as this will become the bottleneck and if it's pegged at 100% then that will end up slower so it'll be better to use fewer processes. (The compression algorithm is multithreaded when saving, but don't really need to worry about that). A pool size of 4 is probably as big as is worthwhile on an average PC especially if the source files are being read over a network or spinning disk.\n",
    "\n",
    "https://medium.com/@grvsinghal/speed-up-your-python-code-using-multiprocessing-on-windows-and-jupyter-or-ipython-2714b49d6fac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# due to some weird reason this function needs to be defined externally and imported \n",
    "#def callAgg(f):\n",
    "#    try:\n",
    "#        agg = SpatialAggregator([f], outDir, ndvOut, stats, aggArgs)\n",
    "#        agg.RunAggregation()\n",
    "#    except KeyboardInterrupt, e:\n",
    "#        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from spatial_agg_worker import callAgg\n",
    "\n",
    "# now we can just do this:\n",
    "#p = Pool(4)\n",
    "#p.map_async(callAgg, inContFiles)\n",
    "# but it is impossible to interrupt if we need to! to allow that, need to do this:\n",
    "    # https://bryceboe.com/2010/08/26/python-multiprocessing-and-keyboardinterrupt/\n",
    "\n",
    "# as we have to import callAgg it won't have access to the globals outDir etc so zip them with the filename \n",
    "# into a single object that can be passed via pool.map\n",
    "zippedArgs = [(f,outDir,ndvOut,stats,aggArgs) for f in inContFiles]\n",
    "def runMulti():\n",
    "    # choose an number not greater than the number of cores, but also that won't use more than \n",
    "    # the available memory and preferably substantially less so that OS-level write-caching can \n",
    "    # help prevent the disk becoming a bottleneck (ensure you are writing to a disk with write\n",
    "    # caching enabled: it isn't by default on external drives)\n",
    "    pool = Pool(4)\n",
    "    p = pool.map_async(callAgg, zippedArgs)\n",
    "    try:\n",
    "        r = p.get(0xFFFF)\n",
    "    except KeyboardInterrupt:\n",
    "        print (\"parent received interrupt\")\n",
    "        return\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# call it\n",
    "runMulti()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
